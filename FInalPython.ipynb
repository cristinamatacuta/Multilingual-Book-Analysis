{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1gY0QvBjvYa_DMu7bdLyuevrr3mZYYCp4","authorship_tag":"ABX9TyOAIvL3QZiqCNTShBtA4IBK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Download the necessary libraries"],"metadata":{"id":"bB4ELWIOgNsH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fB-y3E7qgG1V","executionInfo":{"status":"ok","timestamp":1706212627398,"user_tz":-60,"elapsed":9330,"user":{"displayName":"Cristina Matacuta","userId":"06971792619602549271"}},"outputId":"fe5dc565-63b9-46fa-8c31-d24c6f426658"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["!pip install pandas seaborn transformers matplotlib  nltk"]},{"cell_type":"markdown","source":["\n","### Import the Libraries\n","The modules punkt and stop words from NLTK will also be downloaded"],"metadata":{"id":"WJ6X46OcgdcR"}},{"cell_type":"code","source":["\n","import nltk\n","# Download the 'punkt' resource\n","nltk.download('punkt')\n","# Download stop words\n","nltk.download('stopwords')\n","from nltk import word_tokenize, sent_tokenize\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import string\n","import pandas as pd\n","import seaborn as sns\n","from transformers import pipeline\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzP7k33agln0","executionInfo":{"status":"ok","timestamp":1706212657245,"user_tz":-60,"elapsed":13762,"user":{"displayName":"Cristina Matacuta","userId":"06971792619602549271"}},"outputId":"4bb31d00-e97e-4f63-f899-7677f261882a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["### Read the files"],"metadata":{"id":"ngPG30RNg5VF"}},{"cell_type":"code","source":["def read_book(filename):\n","    \"\"\"\n","    Function that reads the content of the books\n","\n","    Args: filename(str): The filename of the file that needs to be read\n","          language(str): The language of the text\n","\n","    Returns: text(str): A string containing the text\n","    \"\"\"\n","    # Open the file and save it in a variable\n","    with open(filename, \"r\", encoding=\"utf-8\") as file_to_read:\n","        text = file_to_read.read()\n","        # Return the Variable\n","        return text"],"metadata":{"id":"MzXomXf9g_b2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Sentence tokenization\n","This function is used as a helper function for the sentiment analysis function"],"metadata":{"id":"d3s964_MgE0a"}},{"cell_type":"code","source":["def sentence_fragmentation(text):\n","  \"\"\"This function takes the text and breaks it into sentences. Although the function is not necesary for the overall purpose of this project,\n","  it is a helper function for the sentiment analysis.\n","\n","  Args: text(str): The text that needs to be broke into sentences\n","\n","  Returns: sentences(lst): A list containing all the sentences\"\"\"\n","  # Break the text into sentences\n","  sentences=nltk.sent_tokenize(text)\n","  # Return the list of sentences\n","  return sentences"],"metadata":{"id":"lkv7CCR1gNAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Word Tokenization using nltk.word_tokenize"],"metadata":{"id":"lXSiR7OVgLXl"}},{"cell_type":"code","source":["\n","def word_fragmentation(text):\n","    \"\"\"\n","    Function that tokenize the words. Part of the Lexical Analysis in NLP\n","\n","    Args: text(str): The text that will be tokenized\n","\n","    Returns: words(lst): A list of all the words in the text\n","    \"\"\"\n","    # Tokenize the words\n","    words = nltk.word_tokenize(text)\n","    # Return the words\n","    return words\n"],"metadata":{"id":"gmZQty_VhHc-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Text Normalization\n","The text normalization regions starts here. The tokenized words will be lowerd cased, punctuation and stop words will be removed"],"metadata":{"id":"GlyEBmm0hK_9"}},{"cell_type":"markdown","source":["***Remove Punctuation***"],"metadata":{"id":"P-8iAi6RhiYV"}},{"cell_type":"code","source":["def remove_punctuation(text):\n","    \"\"\"\n","    Function that removes the punctuation using the string punctuation method. Due to the limitation of it, customization was required in order to eliminated laguage specific punctuation\n","    Args: text(str): The original text from where punctuation will be removed\n","\n","    Returns: words_without_punctuation(lst): A list of words without punctuation\n","    \"\"\"\n","    # Call the function that tokenizes the text into words\n","    words = word_fragmentation(text)\n","    # Create a customer punctuation to deal with special caracthers that are  language specific\n","    custom_punctuation = \"’“”¿¡„ ” — ''``\"\n","    # Concatinated the string punctuation with the custom punctuation and save in a variable\n","    punctuation = string.punctuation + custom_punctuation\n","    # Save the words without punctuation as being each word that is not presented in the punctuation variable\n","    words_without_punctuation = [word for word in words if word not in punctuation]\n","\n","    # Return the words without punctuation\n","    return words_without_punctuation\n"],"metadata":{"id":"S1Hp87N8hKCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Lower Case Words***"],"metadata":{"id":"nmL3XSfJholM"}},{"cell_type":"code","source":["def small_letters(text):\n","    \"\"\"\n","    Function that lower cases all words in the text. Part of the Normalization process in NLP.\n","\n","    Args: text(str): The text that will be lowered cased\n","\n","    Returns: lowered_cased_words(lst): A list of all words lowered cased\n","    \"\"\"\n","    # Call the word_fragmenation function and save the result in a variable\n","    words = remove_punctuation(text)\n","    # Lower case all the words\n","    lowered_cased_words = [word.lower() for word in words]\n","    # Return the lowered case words\n","    return lowered_cased_words"],"metadata":{"id":"hShh6lsWhchX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Remove Stop Words. Language Specific function.***"],"metadata":{"id":"5YeF-r50huy0"}},{"cell_type":"code","source":["def remove_stop_words(text, language):\n","    \"\"\"\n","    Function that removes the stop words. Part of the Lexical Analysis/Normalization in NLP\n","\n","    Args: text(str): The original text that needs to be analyzed\n","          language(str): The language in which the text is written\n","\n","    Returns: filtered_words(lst): A list of words except stop words\n","\n","    Raises: ValueError: If inputed language is not accepted\n","    \"\"\"\n","    # Tokenize the words using the small_letters function\n","    # Considering that the function does not need to be case sansitive small_letters can be used\n","    words = small_letters(text)\n","\n","    # If conditional to decide on the set of stop words\n","    # If language is English use the English StopWords\n","    if language.lower() == \"english\":\n","        # Save the stop words in a variable\n","        stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n","\n","    # If language is Spanish use the Spanish stopwords\n","    elif language.lower() == \"spanish\":\n","        # Save the stop words in a variable\n","        stop_words = set(nltk.corpus.stopwords.words(\"spanish\"))\n","\n","    # If language is Romanian use the Romanian stop words\n","    elif language.lower() == \"romanian\":\n","        # Save the stop words in a variable\n","        stop_words = set(nltk.corpus.stopwords.words(\"romanian\"))\n","    # If language not Romanian/English/Spanish rise error and print message\n","    else:\n","        raise ValueError(\"Language can only be: english, spanish or romanian\")\n","\n","    # Take out the stopwords from the words\n","    # Save the filtered words in a variable\n","    filtered_words = [word for word in words if word not in stop_words]\n","\n","    # Return the filtered words\n","    return filtered_words"],"metadata":{"id":"AZLMzv7EhgMm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vocabulary richness.\n","The richness of the vocabulary will be calculated using the formula: type-token ratio = (number of types/number of tokens)"],"metadata":{"id":"JqFaEdCLiiWD"}},{"cell_type":"markdown","source":["***Extract types/unique words***"],"metadata":{"id":"zhOJr8PWirDh"}},{"cell_type":"code","source":["def unique_words(text):\n","    \"\"\"\n","    Function that looks for the unique words in the text. It ignores the capital letters and considers words like \"Hello\", and \"hello\" as the same words\n","\n","    Args:   text(str): The text that is analyzed\n","\n","\n","    Returns: unique_words(set): A set of unique words\n","    \"\"\"\n","    # Call the method and save it in a variable\n","    words = small_letters(text)\n","    # Create a set of unique words\n","    unique_words = set(words)\n","    # Return the set of unique words\n","    return unique_words"],"metadata":{"id":"TWNxAafnidw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Calculate the vocabulary richness***"],"metadata":{"id":"OuT-CagEi3an"}},{"cell_type":"code","source":["def vocabulary_richness(text):\n","    \"\"\"\n","    This function determines the vocabulary richness of each book by using the Type-Token-Ration forumla\n","\n","    Args: text(str): The text that will be analyzed\n","\n","\n","    Returns: vocab_richness(int): An integer represing the vocabulary richness\n","    \"\"\"\n","    # Call the function that returns all words and save it in a variable\n","    all_words = small_letters(text)\n","    # Call the function that returns only the unique words and save the result in a variable\n","    set_words = unique_words(text)\n","    # Calculate TTR using the forumla\n","    vocab_richness = len(set_words) / len(all_words) * 100\n","    # Return the vocabulary richness as an integer\n","    return vocab_richness"],"metadata":{"id":"9vCgfIEWi2iX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualization\n","This project aims to extract the most used and vizualize them in a barchart using Seaborn and Pandas. First the top 10 words will be extracted and saved in a DataFrame, then they will be vizualized.\n"],"metadata":{"id":"IzpKZhlhiHsd"}},{"cell_type":"markdown","source":["***Extract Top 10 Words***"],"metadata":{"id":"FRZKfK-jiKtY"}},{"cell_type":"code","source":["\n","def word_frequency(text, language):\n","    \"\"\"\n","    Function that counts the 10 most used words in each text.\n","\n","    Args: text(str): The text that needs to be analyzed\n","          language(str): The language in which the text is written\n","\n","    Returns: df(dataFrame): A dataframe containing the words and their frequency\n","    \"\"\"\n","    # Call the function that removes the stop words\n","    # This returns a list of words without stop words, punctuation and all lower cased\n","    words = remove_stop_words(text, language)\n","\n","    # Create a counter object\n","    word_counts = Counter(words)\n","\n","    # Get the top 10 used words and save in a dictionary\n","    top_10_words = dict(word_counts.most_common(10))\n","\n","    # Pass the dictionary to a DataFrame\n","    # Add Column names to the DataFrame\n","    df = pd.DataFrame(top_10_words.items(), columns=[\"Word\", \"Frequency\"])\n","\n","    # Return the top 10 words\n","    return df"],"metadata":{"id":"M1N97QeJh52N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Plot the Visualization, save the image and show it.***"],"metadata":{"id":"V7jFZFjPiTMZ"}},{"cell_type":"code","source":["def visualization_bar_chart(top_ten_original, top_ten_translation, figure_name, language):\n","    \"\"\"\n","    Creates a bar chart visualization using seaborn containing the top 10 most used words in the Original Version and the Romanian Translation together\n","\n","    Args: top_ten_original(Dataframe): The dataframe containing the words in the original version\n","          top_ten_translation(Dataframe): The dataframe containing the words in the romanian translation\n","          figure_name(str): The name under which the figure will be saved\n","          language(str): The language of the original book\n","\n","    \"\"\"\n","\n","    # Plot a 2 way figure\n","    figure, axis = plt.subplots(1, 2)\n","\n","    # Plot the first figure for the original text\n","    bar_chart1 = sns.barplot(ax=axis[0], data=top_ten_original, x=\"Word\", y=\"Frequency\")\n","\n","    # Rotate the labels for better visibility\n","    bar_chart1.set_xticklabels(top_ten_original[\"Word\"], rotation=90)\n","\n","    # Set a title\n","    bar_chart1.set_title(f\"{language} Original\")\n","\n","    # Label X and Y axis\n","    bar_chart1.set_xlabel(\"Word\")\n","    bar_chart1.set_ylabel(\"Frequency\")\n","\n","    # Plot the second figure for the Romanian stranslation\n","    bar_chart2 = sns.barplot(\n","        ax=axis[1], data=top_ten_translation, x=\"Word\", y=\"Frequency\"\n","    )\n","\n","    # Rotate the labels for better visibility\n","    bar_chart2.set_xticklabels(top_ten_translation[\"Word\"], rotation=90)\n","\n","    # Set a title\n","    bar_chart2.set_title(\" Romanian\")\n","    # Label X and Y axis\n","    bar_chart2.set_xlabel(\"Word\")\n","    bar_chart2.set_ylabel(\"Frequency\")\n","\n","    # Save the figure in a png format\n","    # Increse the Quality of the Image\n","    plt.savefig(figure_name, dpi=300)\n","    # Show the figure\n","    plt.show()"],"metadata":{"id":"oqOE03bGiSnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentiment/Emotion Analysis\n","Credits to [Daveni](https://huggingface.co/daveni/twitter-xlm-roberta-emotion-es) for the Spanish Emotion Model, [Bhadresh-Savani](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) for the English Emotion Model, [Alexandra Ciobanu](https://github.com/Alegzandra) for the data sets provided, and to [Venelin Valkov](https://www.youtube.com/channel/UCoW_WzQNJVAjxo4osNAxd_g) for the wonderful and easy to follow tutorial and explanations."],"metadata":{"id":"CGVR7JKOjLHj"}},{"cell_type":"code","source":["\n","def emotion_analysis_first_sentence(text, language):\n","    \"\"\"\n","    Function that showcases the emotion analysis for the first sentence of the text.\n","    Each languages uses a different emotion detection sentiment.\n","    For Spanish credits are given to Daveni, and the sentiments are:\"sadness\", \"joy\", \"anger\",\"surprise\", \"disgust\",\n","    \"fear\", \"others\".\n","    For English credits are due to Bhadresh-Savani, and the sentiments are: \"sadness\", \"joy\", \"love\", \"anger\",\n","    \"fear\", \"surprise\".\n","    For Romanian I have developed my own model using Venelin Valkov's tutorial and Alexandra's Ciobanu datasets.\n","    Please see more details in the report of the code. The sentiments are: \"Neutral\", \"Joy\", \"Anger\", \"Fear\",\n","    \"Sadness\".\n","\n","    Args: text(str): The text that needs to be analyzed\n","          language(str): The language of the text that needs to be analyzed\n","\n","    Returns: emotion(dict): The emotion label and probability\n","\n","    \"\"\"\n","\n","\n","    # If conditional for language decision\n","    if language.lower() == \"spanish\":\n","        # If language is spanish use XLM Roberta model from Daveni\n","        pipe = pipeline(\n","            \"text-classification\", model=\"daveni/twitter-xlm-roberta-emotion-es\"\n","        )\n","    elif language.lower() == \"english\":\n","        # If language is english use Bert model from Bhadresh Savani\n","        pipe = pipeline(\n","            \"text-classification\",\n","            model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n","        )\n","    elif language.lower() == \"romanian\":\n","        # If language is romanian use bert model\n","        pipe = pipeline(\"text-classification\", model=\"cristinaale/romanianlastemotion1\")\n","\n","\n","\n","    # Call the method that returns a list of sentences from the text and save into a variable\n","    sentences=sentence_fragmentation(text)\n","    # Get the first sentence\n","    first_sentence = sentences[:1]\n","\n","    # Apply emotion analysis to the first sentence\n","    emotion=pipe(first_sentence)\n","    # Return the emotion\n","    return emotion\n"],"metadata":{"id":"qFgxB0GMdT8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Function of the program.\n","In the main the printing will take place, and each language/book pair will be analyzed together in chuncks of code\n","  "],"metadata":{"id":"KpFfgCSCkjAh"}},{"cell_type":"code","source":["def main():\n","\n","\n","    # Define variables where to save the language for easier usage\n","    language1 = \"spanish\"\n","    language2 = \"romanian\"\n","    language3 = \"english\"\n","\n","    # Spanish-Romanian Comparasion\n","\n","    # Save the Spanish path of the book\n","    book_spanish = \"/content/drive/MyDrive/Python_Final/spanish_text.txt\"\n","    # Save the Romanian Translation of the Spanish book into a Variable\n","    book_romanian = \"/content/drive/MyDrive/Python_Final/romanian_spanish.txt\"\n","\n","    # Read the books by calling the function\n","\n","    # Read Spanish book\n","    text_spanish = read_book(book_spanish)\n","\n","    # Read Romanian translation of the Spanish book\n","    text_romanian = read_book(book_romanian)\n","\n","\n","    # Return the number of words for each text by calling the method and applying the len method to the return\n","    words_spanish=remove_punctuation(text_spanish)\n","    print(\"The Spanish Book has:\", len(words_spanish), \"words\")\n","\n","    words_romanian=remove_punctuation(text_romanian)\n","    print(\"The Romanian book has:\", len(words_romanian), \"words\")\n","\n","    # Aestetics blankspace\n","    print()\n","\n","    # Calculate the vocabulary richness of the books by calling the function and adding the language variable\n","    # Spanish Vocabulary richness\n","    spanish_vocabulary_richness = vocabulary_richness(text_spanish)\n","\n","    # Romanian Translation richness\n","    romanian_vocabulary_richness = vocabulary_richness(text_romanian)\n","\n","    # Print the vocabulary richness integers\n","    # Print the Spanish vocabulary richness\n","    print(\"The Spanish text has a vocabulary richness of\", spanish_vocabulary_richness)\n","    # Blankspace for aestetics\n","    print()\n","\n","    # Print the Romanian Translation Vocabulary richness\n","    print(\n","        \"The Romanian text has a vocabulary richness of\", romanian_vocabulary_richness\n","    )\n","    # Blanckspace for aestetics\n","    print()\n","\n","    # Top 10 words by calling the function\n","    # DataFrame containing the top 10 most used words in the Spanish book\n","    spanish_top_10 = word_frequency(text_spanish, language1)\n","\n","    # DataFrame containing the top 10 most used words in the Romanian translation of the Spanish book\n","    romanian_top_10 = word_frequency(text_romanian, language2)\n","\n","    # Create the vizualization for Spanishand Romanian top 10 words by calling the function\n","    # That uses the spanish_top_10 and romanian_top_10 dataframes\n","    visualization_bar_chart(spanish_top_10, romanian_top_10, \"romanian_spanish.png\", language1)\n","\n","\n","    # Get the sentiment analysis of the 100 lenght chunks\n","    # Get the sentiment analysis for the Spanish book\n","    spanish_sentiment = emotion_analysis_first_sentence(text_spanish, language1)\n","\n","    # Get the sentiment analysis for the Romanian translation of the Spanish book\n","    romanian_sentiment = emotion_analysis_first_sentence(text_romanian, language2)\n","\n","    # Print the sentiment analysis results for each language\n","    # Print the sentiment analysis for the Spanish book\n","    print(\"The Spanish Sentiment Analysis:\", spanish_sentiment)\n","\n","    # Print the sentiment analysis result for the Romanian translation of the Spanish book\n","    print(\"The Romanian Sentiment Anaalysis:\", romanian_sentiment)\n","\n","    # English Romanian Comparision\n","\n","    # Save the paths of the books\n","    # Save the path of the English book\n","    book_english = \"/content/drive/MyDrive/Python_Final/english_text.txt\"\n","\n","    # Save the path of the Romanian translation of the English book\n","    book_romanian2 = \"/content/drive/MyDrive/Python_Final/romanian_english.txt\"\n","\n","    # Read the books by calling the method\n","    # Read the English text\n","    text_english = read_book(book_english)\n","\n","    # Read the Romanian translation of the English text\n","    text2_romanian = read_book(book_romanian2)\n","\n","    # Return the number of words for each text by calling the method and applying the len method to the return\n","    # Use remove_punctuation function to get the number of words without punctuation\n","    words_english=remove_punctuation(text_english)\n","    print(\"The English Book has:\", len(words_english), \"words\")\n","\n","    words_romanian2=remove_punctuation(text2_romanian)\n","    print(\"The Romanian book has:\", len(words_romanian2), \"words\")\n","\n","    # Calculate the vocabulary richness of the books by calling the function\n","    # The English text vocabularry richness\n","    english_vocabulary_richness = vocabulary_richness(text_english)\n","\n","    # The Romanian translation vocabulary richness\n","    romanian_vocabulary_richness2 = vocabulary_richness(text2_romanian)\n","\n","    # Blankspace for aestetics\n","    print()\n","\n","    # Print the vocabulary richness integers\n","    # Print the English vocabulary richness\n","    print(\"The English text has a vocabulary richness of\", english_vocabulary_richness)\n","    # Blankspace for aesetics\n","    print()\n","\n","    # Print the Romanian vocabulary richness\n","    print(\n","        \"The Romanian text has a vocabulary richness of\", romanian_vocabulary_richness2\n","    )\n","    # Blank Space for Aestetics\n","    print()\n","\n","    # Top 10 most used words in the book\n","    # DataFrame containing the top 10 most used words in the English book\n","    english_top_10 = word_frequency(text_english, language3)\n","\n","    # DataFrame containing the top 10 most used words in the Romanian translation\n","    romanian2_top_10 = word_frequency(text2_romanian, language2)\n","\n","    # Create the vizualization for Spanishand Romanian top 10 words by calling the function\n","    # That uses the english_top_10 and romanian_top_10\n","    visualization_bar_chart(english_top_10, romanian2_top_10, \"romanian_english.png\", language3)\n","\n","\n","    # Get the sentiment analysis of the 100 lenght chunks\n","    # Get the English book sentiment\n","    english_sentiment = emotion_analysis_first_sentence(text_spanish, language3)\n","    romanian_sentiment2 = emotion_analysis_first_sentence(text2_romanian, language2)\n","\n","    # Print the sentiment analysis results for each language\n","    # Print the English sentiment\n","    print(\"The English Sentiment Analysis:\", english_sentiment)\n","    print(\"The Romanian Sentiment Anaalysis:\", romanian_sentiment2)\n","\n"],"metadata":{"id":"YTa3Um8wkhtj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calling the Main"],"metadata":{"id":"mMORfkvTlF0J"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"xgiyLgy0lE3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jSmHsyxOlJEL"},"execution_count":null,"outputs":[]}]}